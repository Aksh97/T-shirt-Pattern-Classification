{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============================T-Shirt Classifier=============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import PIL\n",
    "import csv\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.inception_v3 import InceptionV3, decode_predictions\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walking the directory to get all image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "\n",
    "train_files = os.listdir('train_images')\n",
    "val_files = os.listdir('val_images')\n",
    "test_files = os.listdir('test_images') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying 6 random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Temp = np.random.choice(train_files, 6)\n",
    "for i in Temp:\n",
    "    Label = i.split('_')[0]    \n",
    "    im = plt.imread('train_images/'+i)\n",
    "    plt.imshow(im)\n",
    "    plt.title(Label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 256\n",
    "def PreprocessingImages(files,directory):\n",
    "    x = []\n",
    "    print('Preprocessing '+directory+' images ...')\n",
    "    for file in files:\n",
    "        Imgpath = directory+'_images/'+file\n",
    "        im = Image.open(Imgpath)\n",
    "        im = im.resize((size, size), PIL.Image.NEAREST)\n",
    "        im = np.asarray(im, dtype='float64')\n",
    "        x.append(im)\n",
    "    print('Total number of '+directory+' images:', len(files))\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = PreprocessingImages(train_files, 'train')\n",
    "x_val = PreprocessingImages(val_files, 'val')\n",
    "x_test = PreprocessingImages(test_files, 'test')\n",
    "print(\"Done reading all images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "print(x_train.shape, 'Train Data Shape')\n",
    "\n",
    "x_val = np.array(x_val)\n",
    "print(x_val.shape, 'Val Data Shape')\n",
    "\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "print(x_test.shape, 'Test Data Shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the into .npy files to load quickly from next time\n",
    "np.save(\"train_preprocessed.npy\",x_train)\n",
    "np.save(\"val_preprocessed.npy\",x_val)\n",
    "np.save(\"test_preprocessed.npy\",x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255.\n",
    "x_val /= 255.\n",
    "x_test /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the into .npy files to load quickly from next time\n",
    "np.save(\"train_normalized.npy\",x_train)\n",
    "np.save(\"val_normalized.npy\",x_val)\n",
    "np.save('test_normalized.npy',x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading the saved files directly\n",
    "x_train = np.load('train_normalized.npy')\n",
    "x_val = np.load('val_normalized.npy')\n",
    "x_test = np.load('test_normalized.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the labels from name of each images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLabel(files):\n",
    "    y = []\n",
    "    for file in files:\n",
    "        y.append(file.split('_')[0])\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = GetLabel(train_files)\n",
    "y_val = GetLabel(val_files)\n",
    "y_test = GetLabel(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the labels(from text to numbers)\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val = le.fit_transform(y_val)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "keys = le.classes_\n",
    "values = le.transform(le.classes_)\n",
    "dictionary = dict(zip(keys, values))\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting labels to categorical data\n",
    "y_train = to_categorical(y_train)\n",
    "print(y_train.shape, 'Train data Labels Shape')\n",
    "\n",
    "y_val = to_categorical(y_val)\n",
    "print(y_val.shape, 'Val data Labels Shape')\n",
    "\n",
    "y_test = to_categorical(y_test)\n",
    "print(y_test.shape, 'Test data Labels Shape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building model using Transfer Learning(Inception V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = InceptionV3(include_top=False,weights='imagenet')\n",
    "\n",
    "x = trained_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128,activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "pred_inception= Dense(2,activation='sigmoid')(x)\n",
    "model = Model(inputs=trained_model.input,outputs=pred_inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the layers of inception non-trainable\n",
    "for layer in trained_model.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "#compiling the model\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size= 32, epochs= 25, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelV1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(filters=32, kernel_size=(3, 3), padding='same', \n",
    "                        input_shape=(256,256,3), \n",
    "                        activation='relu'))\n",
    "model.add(Convolution2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Convolution2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def model_train(optimizer, lr_rate, freez, nb_epoch=2):\n",
    "    tag = 'model_'+optimizer+'_lr_'+str(lr_rate)+'_freeze_'+str(freez)\n",
    "    trained_model = InceptionV3(include_top=False,weights='imagenet')\n",
    "    x = trained_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred_inception= Dense(2,activation='sigmoid')(x)\n",
    "    model = Model(inputs=trained_model.input,outputs=pred_inception)\n",
    "\n",
    "    #making the layers non-trainable\n",
    "    for layer in trained_model.layers:\n",
    "        layer.trainable=freez\n",
    "\n",
    "    #compiling the model\n",
    "    if optimizer=='adam':\n",
    "        optim = Adam(lr=lr_rate)\n",
    "    elif optimizer=='sgd':\n",
    "        optim = SGD(lr=lr_rate)\n",
    "    model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer=optim)\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale=None, \n",
    "                                       height_shift_range=0.2, \n",
    "                                       width_shift_range=0.2,\n",
    "                                       horizontal_flip=True, \n",
    "                                       rotation_range=10,shear_range=0.2,zoom_range=0.2,)\n",
    "    validation_datagen = ImageDataGenerator(rescale=None)\n",
    "\n",
    "    train_generator = train_datagen.flow(x=x_train, y=y_train,batch_size=batch_size)\n",
    "    validation_generator = validation_datagen.flow(x=x_val, y=y_val,batch_size=batch_size)\n",
    "\n",
    "    hist = model.fit_generator(train_generator,\n",
    "                        steps_per_epoch=train_generator.n//batch_size, # '//' in python returns only the quotient\n",
    "                        epochs=nb_epoch,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=validation_generator.n//batch_size, \n",
    "                        verbose=0).history\n",
    "    \n",
    "    model.save(tag+'.h5')\n",
    "    np.save(tag+'.npy', hist)\n",
    "    \n",
    "    t_acc = np.min(hist['acc'])\n",
    "    v_acc = np.min(hist['val_acc'])\n",
    "    return(t_acc, v_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_list = ['adam', 'sgd']\n",
    "lr_rate_list = [0.01,0.001]\n",
    "freez_list = [True, False]\n",
    "\n",
    "gs_list = list(itertools.product(optimizer_list, lr_rate_list, freez_list))\n",
    "print(gs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_acc = []\n",
    "val_acc = []\n",
    "for op, lr, fr in gs_list:\n",
    "    print('Running combination - Optimizer:{}, Learning Rate:{}, Freeze Weights:{}'.format(op, lr, fr))\n",
    "    r1, r2 = model_train(op, lr, fr)\n",
    "    tr_acc.append(r1)\n",
    "    val_acc.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_adam_lr_0.01_freeze_True.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(gs_list)\n",
    "df.columns = ['Optimizer', 'Learning Rate', 'Freeze Weights']\n",
    "df['Train Acc'] = tr_acc\n",
    "df['Val Acc'] = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
